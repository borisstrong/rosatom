{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final-CCT-inference.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj1J9mgHgI2_",
        "outputId": "778c480b-4727-4347-a1c7-76baf47c8e19"
      },
      "source": [
        "!pip install eo-learn -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 10.5 MB 7.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 69 kB 9.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 131 kB 64.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 201 kB 65.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 48.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 66.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 53.8 MB 255 kB/s \n",
            "\u001b[K     |████████████████████████████████| 19.3 MB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 994 kB 40.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 15.4 MB 40.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 46.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 34.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.4 MB 27.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 128 kB 75.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 68.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47 kB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 44 kB 3.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 51.6 MB/s \n",
            "\u001b[?25h  Building wheel for sentinelhub (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for thunder-registration (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for thunder-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for bolt-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for utm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-ViEYcbgO_Q"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os, glob\n",
        "import cv2\n",
        "import eolearn\n",
        "import gdown\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "\n",
        "from eolearn.core import EOPatch, FeatureType"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb_-394cgQu-"
      },
      "source": [
        "TARGET_SIZE = (64, 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2zMbqF0gVOL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1EZpGMugiNd"
      },
      "source": [
        "## Новый раздел"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np7MEi95RiZF"
      },
      "source": [
        "def read_file(filepath):\n",
        "    eopatch = EOPatch.load(filepath)\n",
        "    img = eopatch.data['L2A']\n",
        "    masks = [eopatch.mask[x] for x in eopatch.mask]\n",
        "\n",
        "    return np.concatenate([img] + masks, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkfwXUvyglE6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv1Aq-oygsE8"
      },
      "source": [
        "## CCT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCTCprJ0L1iD"
      },
      "source": [
        "### Гиперпараметры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu-hdijUL1iE"
      },
      "source": [
        "positional_emb = True\n",
        "conv_layers = 2\n",
        "projection_dim = 128\n",
        "\n",
        "num_heads = 2\n",
        "transformer_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "]\n",
        "transformer_layers = 2\n",
        "stochastic_depth_rate = 0.1\n",
        "\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 128\n",
        "num_epochs = 30\n",
        "image_size = 64\n",
        "input_shape = [64, 64, 19]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2cYPG7xL1iG"
      },
      "source": [
        "### Токенайзер"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk8CzUJVL1iG"
      },
      "source": [
        "\n",
        "class CCTTokenizer(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1,\n",
        "        pooling_kernel_size=3,\n",
        "        pooling_stride=2,\n",
        "        num_conv_layers=conv_layers,\n",
        "        num_output_channels=[64, 128],\n",
        "        positional_emb=positional_emb,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(CCTTokenizer, self).__init__(**kwargs)\n",
        "\n",
        "        self.conv_model = keras.Sequential()\n",
        "        for i in range(num_conv_layers):\n",
        "            self.conv_model.add(\n",
        "                layers.Conv2D(\n",
        "                    num_output_channels[i],\n",
        "                    kernel_size,\n",
        "                    stride,\n",
        "                    padding=\"valid\",\n",
        "                    use_bias=False,\n",
        "                    activation=\"relu\",\n",
        "                    kernel_initializer=\"he_normal\",\n",
        "                )\n",
        "            )\n",
        "            self.conv_model.add(layers.ZeroPadding2D(padding))\n",
        "            self.conv_model.add(\n",
        "                layers.MaxPool2D(pooling_kernel_size, pooling_stride, \"same\")\n",
        "            )\n",
        "\n",
        "        self.positional_emb = positional_emb\n",
        "\n",
        "    def call(self, images):\n",
        "        outputs = self.conv_model(images)\n",
        "        # После прохождения данной мини-сетки изображение превращается в ряд последовательностей\n",
        "        reshaped = tf.reshape(\n",
        "            outputs,\n",
        "            (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[-1]),\n",
        "        )\n",
        "        return reshaped\n",
        "\n",
        "    def positional_embedding(self, image_size):\n",
        "        # Опциональная часть с позиционным эмбеддингом\n",
        "        if self.positional_emb:\n",
        "            dummy_inputs = tf.ones((1, image_size, image_size, 3))\n",
        "            dummy_outputs = self.call(dummy_inputs)\n",
        "            sequence_length = tf.shape(dummy_outputs)[1]\n",
        "            projection_dim = tf.shape(dummy_outputs)[-1]\n",
        "\n",
        "            embed_layer = layers.Embedding(\n",
        "                input_dim=sequence_length, output_dim=projection_dim\n",
        "            )\n",
        "            return embed_layer, sequence_length\n",
        "        else:\n",
        "            return None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOx0UDVoL1iI"
      },
      "source": [
        "### Stochastic depth\n",
        "\n",
        "[Stochastic depth](https://arxiv.org/abs/1603.09382) -\n",
        "техника регуляризации, которая случайным образом блокирует работу некоторых слоев нейросети. По смыслу близка к \"дропауту\" - [Dropout](https://jmlr.org/papers/v15/srivastava14a.html), с той разницей, что дропаут блокирует отдельные нейроны."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1IaxUOPL1iJ"
      },
      "source": [
        "# Источник: github.com:rwightman/pytorch-image-models.\n",
        "class StochasticDepth(layers.Layer):\n",
        "    def __init__(self, drop_prop, **kwargs):\n",
        "        super(StochasticDepth, self).__init__(**kwargs)\n",
        "        self.drop_prob = drop_prop\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        if training:\n",
        "            keep_prob = 1 - self.drop_prob\n",
        "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
        "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
        "            random_tensor = tf.floor(random_tensor)\n",
        "            return (x / keep_prob) * random_tensor\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8rk_EQkL1iJ"
      },
      "source": [
        "### multilayer perceptron (MLP) для энкодера трансформера"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uceGHiOeL1iK"
      },
      "source": [
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz9RjQA5L1iL"
      },
      "source": [
        "### Финальная сборка модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUu1X7bXL1iL"
      },
      "source": [
        "\n",
        "def create_cct_model(\n",
        "    input_shape=input_shape,\n",
        "    image_size=image_size,\n",
        "    num_heads=num_heads,\n",
        "    projection_dim=projection_dim,\n",
        "    transformer_units=transformer_units,\n",
        "):\n",
        "\n",
        "    inputs = layers.Input(input_shape)\n",
        "\n",
        "    # Кодирование (нарезка) патчей.\n",
        "    cct_tokenizer = CCTTokenizer()\n",
        "    encoded_patches = cct_tokenizer(inputs)\n",
        "\n",
        "    # Применение позиционного эмбеддинга.\n",
        "    \n",
        "    # Stochastic Depth\n",
        "    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
        "\n",
        "    # Блоки трансформера.\n",
        "    for i in range(transformer_layers):\n",
        "        # Нормализация.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
        "\n",
        "        # Self-Attention-блок.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "\n",
        "        # Skip-connection.\n",
        "        attention_output = StochasticDepth(dpr[i])(attention_output)\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "        # Нормализация.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
        "\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "\n",
        "        # Skip-connection.\n",
        "        x3 = StochasticDepth(dpr[i])(x3)\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Применение sequence pooling для получения взвешенного выхода сети.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
        "    attention_weights = tf.nn.softmax(layers.Dense(1,)(representation), axis=1)\n",
        "    weighted_representation = tf.matmul(\n",
        "        attention_weights, representation, transpose_a=True\n",
        "    )\n",
        "    weighted_representation = tf.squeeze(weighted_representation, -2)\n",
        "\n",
        "    # Выход классификатора.\n",
        "    logits = layers.Dense(1,  activation='sigmoid')(weighted_representation)\n",
        "    # Готовая модель\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TKexAEEg6NH"
      },
      "source": [
        "# Новый раздел"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv3LmEkQg5uN"
      },
      "source": [
        "model = create_cct_model(input_shape = [64, 64, 19])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjF-hJrfg9aB"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=keras.losses.BinaryCrossentropy(),\n",
        "    metrics=[\n",
        "        keras.metrics.Accuracy(name=\"accuracy\")\n",
        "    ],\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slMlO0ibg_HW"
      },
      "source": [
        "model.load_weights('CCT.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE8RqY8qhGoq"
      },
      "source": [
        "## Новый раздел"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGCj4itgjuOm"
      },
      "source": [
        "!unzip -q /content/CCT-inference.zip -d /content/example/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idLC5UEVhFML",
        "outputId": "26d67720-e4c8-4444-93df-6dac817ce71b"
      },
      "source": [
        "filepath = input('input path to folder: ')\n",
        "\n",
        "input_data = read_file(filepath)\n",
        "\n",
        "model(input_data, training=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input path to folder: /content/example/content/spills/2021-01/КБ-2020-1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(28, 1), dtype=float32, numpy=\n",
              "array([[0.00053731],\n",
              "       [0.01329342],\n",
              "       [0.01402196],\n",
              "       [0.01625428],\n",
              "       [0.01519221],\n",
              "       [0.00520554],\n",
              "       [0.03364789],\n",
              "       [0.026039  ],\n",
              "       [0.03156918],\n",
              "       [0.01835027],\n",
              "       [0.0322507 ],\n",
              "       [0.03923708],\n",
              "       [0.02190122],\n",
              "       [0.03197056],\n",
              "       [0.01881793],\n",
              "       [0.02813515],\n",
              "       [0.01391655],\n",
              "       [0.03489986],\n",
              "       [0.03116351],\n",
              "       [0.02107072],\n",
              "       [0.00860626],\n",
              "       [0.00856319],\n",
              "       [0.0082854 ],\n",
              "       [0.01715851],\n",
              "       [0.0142059 ],\n",
              "       [0.00862572],\n",
              "       [0.0087719 ],\n",
              "       [0.00953099]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUWmKItcj2lE",
        "outputId": "c380ffe7-8ce4-479a-9bb8-0269840ce7a3"
      },
      "source": [
        "input_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 64, 64, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}